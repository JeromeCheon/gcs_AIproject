{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2021_10_14_NLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrVrkzEKgNpF"
      },
      "source": [
        "1. 상단 메뉴 -> 런타임-> 런타임 유형 변경 -> GPU\n",
        "2.  transformers 설치\n",
        "3. Reviews.csv 업로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6l_4MLdlSr7S"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fE9MWMm8-yU"
      },
      "source": [
        "!wget -O Reviews.csv https://www.dropbox.com/s/igsnbo24jifkdjr/Reviews_mini.csv?dl=0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QUlrHgSTa4E"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import torch\n",
        "import csv\n",
        "import torchtext\n",
        "import torch.nn as nn\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score as ACC\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohmUe7AYg4zK"
      },
      "source": [
        "df=pd.read_csv('Reviews.csv',error_bad_lines=False, engine=\"python\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSKCTiBcg_85"
      },
      "source": [
        "df = df[['Score','Text']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mykld5I9hExt"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ke8ZwbydOIib"
      },
      "source": [
        "df['Score'].hist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bxbvTsSTfET"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUe27N1lgR5R"
      },
      "source": [
        "tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQqHOavggzaR"
      },
      "source": [
        "example_sentence = df['Text'].iloc[0]\n",
        "example_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfado3NugX1l"
      },
      "source": [
        "tokenizer.encode(example_sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iijPw9qpi4c8"
      },
      "source": [
        "class textDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, tokenizer):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.data['Text'].iloc[idx]\n",
        "        score = self.data['Score'].iloc[idx]\n",
        "\n",
        "        encoded_text = self.tokenizer.encode(text)\n",
        "\n",
        "        encoded_text = torch.tensor(encoded_text).long()\n",
        "        score = torch.tensor(score).long()\n",
        "        score = score-1 # 1~5->0~4\n",
        "        return encoded_text, score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wDJshv-jWiE"
      },
      "source": [
        "split_idx = int(len(df)*0.9)\n",
        "train_data = df.iloc[:split_idx]\n",
        "test_data = df.iloc[split_idx:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEr3fVbTe3Lf"
      },
      "source": [
        "batch_size = 4\n",
        "device = torch.device('cuda')\n",
        "\n",
        "# Data set\n",
        "train_dataset = textDataset(train_data, tokenizer)\n",
        "test_dataset = textDataset(test_data, tokenizer)\n",
        "\n",
        "def collate_fn(batch):\n",
        "  texts, scores = zip(*batch)\n",
        "  texts_pad = pad_sequence(texts, batch_first=True, padding_value=0)\n",
        "  texts_pad = texts_pad[:,:512]\n",
        "  return texts_pad, torch.stack(scores)\n",
        "\n",
        "# Data loader\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=2)\n",
        "test_loader =  torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVLMNB_cjw_H"
      },
      "source": [
        "class simpleRNN(nn.Module):\n",
        "  def __init__(self):\n",
        "        super(simpleRNN, self).__init__()\n",
        "\n",
        "        self.embedding_layer = nn.Embedding(30522, embedding_dim=256)\n",
        "        \n",
        "        self.num_layers = 1\n",
        "        self.RNN = nn.RNN(256, 256, num_layers=self.num_layers, dropout=0.1, batch_first=True)\n",
        "        \n",
        "        self.out = nn.Linear(256, 5)\n",
        "\n",
        "      \n",
        "  def forward(self, text):\n",
        "        x=self.embedding_layer(text)\n",
        "        h0 = torch.zeros(self.num_layers,x.shape[0],256)\n",
        "        if torch.cuda.is_available():\n",
        "            h0 = h0.cuda()\n",
        "        x, h = self.RNN(x, h0)\n",
        "        x = self.out(x[:,-1,:])\n",
        "\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbBcAs6Ff4vd"
      },
      "source": [
        "class BERT(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(BERT, self).__init__()\n",
        "\n",
        "        options_name = \"bert-base-uncased\"\n",
        "        self.bert = BertForSequenceClassification.from_pretrained(options_name, num_labels=5)\n",
        "\n",
        "    def forward(self, text):\n",
        "        x = self.bert(text)[0]\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XK-_2XzfWv3"
      },
      "source": [
        "learning_rate = 1e-5\n",
        "model = BERT()\n",
        "model = simpleRNN()\n",
        "if torch.cuda.is_available():\n",
        "  model = model.cuda()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = torch.optim.Adam(model.bert.classifier.parameters(), lr=learning_rate)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_18TT7IZk873"
      },
      "source": [
        "EPOCHS=1\n",
        "\n",
        "losses = []\n",
        "for epoch in range(EPOCHS):\n",
        "    loss_list = []\n",
        "    acc_list = []\n",
        "\n",
        "    loss_list2 = []\n",
        "    acc_list2 = []\n",
        "    for i, (X_batch, y_batch) in enumerate(train_loader):\n",
        "        if torch.cuda.is_available():\n",
        "            X_batch = X_batch.cuda()\n",
        "            y_batch = y_batch.cuda()\n",
        "        #Forward \n",
        "        y_output = model(X_batch)\n",
        "        loss = criterion(y_output, y_batch) #CELoss: The input is expected to contain raw, unnormalized scores for each class.\n",
        "        \n",
        "        #Backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        #misc (acc 계산, etc) \n",
        "        y_pred = torch.max(y_output, 1)[1]\n",
        "        acc = ACC(y_batch.data.cpu(), y_pred.data.cpu())\n",
        "        loss_list.append(loss.item())\n",
        "        acc_list.append(acc)\n",
        "        loss_list2.append(loss.item())\n",
        "        acc_list2.append(acc)\n",
        "        losses.append(loss.item())\n",
        "        if (i+1) % 20 == 0:\n",
        "            print('Epoch [{}/{}] Step [{}/{}] Loss: [{:.4f}] Train ACC [{:.2f}%]'.format(epoch+1, EPOCHS, \\\n",
        "                                                                                       i+1, len(train_loader), np.mean(loss_list2), np.mean(acc_list2)*100))\n",
        "            loss_list2 = []\n",
        "            acc_list2 = [] \n",
        "    print('Epoch [{}/{}] Loss: [{:.4f}] Train ACC [{:.2f}%]'.format(epoch+1, EPOCHS, np.mean(loss_list), np.mean(acc_list)*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBQ3Zksh95q2"
      },
      "source": [
        "plt.figure(figsize=(8,4))\n",
        "plt.plot(losses)\n",
        "plt.title('Loss graph')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2xMV8PYlDsX"
      },
      "source": [
        "test_acc_list = []\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    for X_batch, y_batch in test_loader:    \n",
        "        if torch.cuda.is_available():\n",
        "            X_batch = X_batch.cuda()\n",
        "            y_batch = y_batch.cuda()\n",
        "        \n",
        "        y_output = model(X_batch)\n",
        "        y_pred = torch.max(y_output, 1)[1]\n",
        "        \n",
        "        acc = ACC(y_batch.data.cpu(), y_pred.data.cpu())\n",
        "        test_acc_list.append(acc)\n",
        "    test_acc = np.mean(test_acc_list)\n",
        "print('Test ACC: [{:.2f}%]'.format(test_acc*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2BbVC0Oaj9m"
      },
      "source": [
        "# 과제\n",
        "\n",
        "63% accuracy를 달성해보세요! (Hint: BERT를 낮은 learning rate로 학습해보세요.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xia316RAaran"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}