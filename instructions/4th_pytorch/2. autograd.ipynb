{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"latex_envs":{"LaTeX_envs_menu_present":true,"autoclose":false,"autocomplete":true,"bibliofile":"biblio.bib","cite_by":"apalike","current_citInitial":1,"eqLabelWithNumbers":true,"eqNumInitial":1,"hotkeys":{"equation":"Ctrl-E","itemize":"Ctrl-I"},"labels_anchors":false,"latex_user_defs":false,"report_style_numbering":false,"user_envs_cfg":false},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"colab":{"name":"2. autograd.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"YaJnSGL1mWWp"},"source":["# Autograd: 자동 미분"]},{"cell_type":"markdown","metadata":{"id":"Xd06or3tmWWs"},"source":["## Autograd Package"]},{"cell_type":"markdown","metadata":{"id":"2aPhOJCGmWWt"},"source":["- Autograd 패키지는 tensor의 모든 연산에 자동 미분을 제공합니다. 이는 define-by-run의 프레임워크로 코드를 어떻게 작성하느냐에 따라 역전파가 정의된다는 뜻입니다. 역전파는 학습과정의 매 단계마다 달라집니다."]},{"cell_type":"markdown","metadata":{"id":"2N53a5rYmWWt"},"source":["- .requires_grad 속성을 True로 설정하면 해당 tensor의 모든 연산을 추적합니다. 계산이 완료된 후 .backward()를 호출해 gradient를 자동으로 계산할 수 있습니다. 이 tensor의 gradient는 .grad에 누적됩니다."]},{"cell_type":"markdown","metadata":{"id":"xZGTpqtumWWu"},"source":["- 연산 기록을 추적하는 것을 멈추기 위해 코드 블럭을 with torch.no_grad():로 감쌀 수 있습니다. gradient는 필요 없지만 requires_grad=True가 설정되어 학습 가능한 Parameter(매개변수)를 갖는 모델을 평가할 때 유용합니다."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2018-10-16T12:34:35.045011Z","start_time":"2018-10-16T12:34:35.041545Z"},"id":"2UTlMmTKmWWu"},"source":["import torch"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mIkX1GzsmWWv"},"source":["# 예시 1"]},{"cell_type":"code","metadata":{"id":"c4fIVcOvmWWw"},"source":["a = torch.randn(2,2)\n","a = ((a*3)/(a-1)) \n","print(a.requires_grad)\n","print(a.grad_fn) # 사용자가 만든 텐서의 grad_fn은 none입니다."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hSUWGmNrmWWx"},"source":["a.requires_grad_(True)\n","print(a.requires_grad)\n","print(a.grad_fn)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u_pSbHZGmWWx"},"source":["b = (a*a).sum()\n","print(b.grad_fn) #requires_grad_(True)로 지정하고 연산하면 이렇게 grad_fn가 생깁니다."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GDyzGoKKmWWy"},"source":["# 예시 2"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2018-10-16T12:34:35.395161Z","start_time":"2018-10-16T12:34:35.390199Z"},"id":"cX9O5Cv8mWWy"},"source":["x = torch.ones(2,2,requires_grad=True) #tensor를 생성하고 requires_grad=True로 연산을 기록합니다.\n","print(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2018-10-16T12:34:35.683683Z","start_time":"2018-10-16T12:34:35.679238Z"},"id":"ProZj579mWWz"},"source":["y = x+2 #gradient function이 자동으로 포함됩니다.\n","print(y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2018-10-16T12:34:35.979952Z","start_time":"2018-10-16T12:34:35.975984Z"},"id":"Bf16-o_xmWWz"},"source":["z = y*y*3\n","out = z.mean()\n","print(z,out)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2tXnV-1lmWWz"},"source":["![](./figures/AUTO1)"]},{"cell_type":"markdown","metadata":{"id":"OFDao2NMmWWz"},"source":["![](./figures2/auto1.PNG)"]},{"cell_type":"markdown","metadata":{"id":"cv5Fy2kcmWW0"},"source":["### Gradient"]},{"cell_type":"markdown","metadata":{"id":"B9AYvpZlmWW1"},"source":["- Autograd 이용 "]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2018-10-16T12:34:37.141890Z","start_time":"2018-10-16T12:34:37.135442Z"},"id":"edBdBHSAmWW1"},"source":["print(out) # out = 3(x+2)*2\n","out.backward()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2018-10-16T12:34:37.738725Z","start_time":"2018-10-16T12:34:37.731296Z"},"id":"lECbNMj2mWW1"},"source":["print(x)\n","print(x.grad) # d(out)/dx 를 출력합니다."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YUNRzaThmWW2"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"teK3eX2bmWW2"},"source":["[링크 텍스트](https://)# 예시 3\n"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2018-10-16T12:47:24.728167Z","start_time":"2018-10-16T12:47:24.724661Z"},"id":"UIhP3ABJmWW3"},"source":["print(x.requires_grad)\n","print((x**2).requires_grad)\n","\n","with torch.no_grad():\n","    print((x**2).requires_grad) #tensor들의 연산 기록 추적을 막을 수 있습니다.\n","     \n","print((x**2).requires_grad)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VYDMfw0umWW3"},"source":["- autograd package에 대한 더 자세한 정보는 다음의 링크를 참고하세요. https://pytorch.org/docs/stable/torch.html"]},{"cell_type":"code","metadata":{"id":"5KCgW91HmWW3"},"source":[""],"execution_count":null,"outputs":[]}]}