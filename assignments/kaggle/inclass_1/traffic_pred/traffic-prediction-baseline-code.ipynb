{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Inclass : Sequential\n",
    "베이스라인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: easydict in /opt/conda/lib/python3.8/site-packages (1.9)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.8/site-packages (8.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install easydict pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorboard\n",
      "  Downloading tensorboard-2.7.0-py3-none-any.whl (5.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.8 MB 13.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.3.0-py2.py3-none-any.whl (154 kB)\n",
      "\u001b[K     |████████████████████████████████| 154 kB 37.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.9 MB 30.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard) (2.26.0)\n",
      "Collecting grpcio>=1.24.3\n",
      "  Downloading grpcio-1.41.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.9 MB 33.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 10.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.8/site-packages (from tensorboard) (0.35.1)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting absl-py>=0.4\n",
      "  Downloading absl_py-0.14.1-py3-none-any.whl (131 kB)\n",
      "\u001b[K     |████████████████████████████████| 131 kB 26.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard) (50.3.1.post20201107)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-2.0.2-py3-none-any.whl (288 kB)\n",
      "\u001b[K     |████████████████████████████████| 288 kB 84.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "\u001b[K     |████████████████████████████████| 781 kB 26.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard) (1.21.2)\n",
      "Collecting protobuf>=3.6.0\n",
      "  Downloading protobuf-3.18.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 34.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 36.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (2.0.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (2021.5.30)\n",
      "Requirement already satisfied: six>=1.5.2 in /opt/conda/lib/python3.8/site-packages (from grpcio>=1.24.3->tensorboard) (1.16.0)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting pyasn1>=0.1.3\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 3.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
      "\u001b[K     |████████████████████████████████| 146 kB 118.0 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pyasn1, rsa, pyasn1-modules, cachetools, google-auth, tensorboard-data-server, grpcio, markdown, oauthlib, requests-oauthlib, google-auth-oauthlib, absl-py, werkzeug, tensorboard-plugin-wit, protobuf, tensorboard\n",
      "Successfully installed absl-py-0.14.1 cachetools-4.2.4 google-auth-2.3.0 google-auth-oauthlib-0.4.6 grpcio-1.41.0 markdown-3.3.4 oauthlib-3.1.1 protobuf-3.18.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.7.2 tensorboard-2.7.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 werkzeug-2.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import argparse\n",
    "import easydict\n",
    "from torch import autograd\n",
    "\n",
    "os.makedirs('log', exist_ok=True)\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter('log/tensorboard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42 #1번째 시도는 2021로 했었음\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = os.path.join('/USER/kaggle/1st_inclass/traffic_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>날짜</th>\n",
       "      <th>시간</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>140</th>\n",
       "      <th>150</th>\n",
       "      <th>160</th>\n",
       "      <th>...</th>\n",
       "      <th>1020</th>\n",
       "      <th>1040</th>\n",
       "      <th>1100</th>\n",
       "      <th>1200</th>\n",
       "      <th>1510</th>\n",
       "      <th>2510</th>\n",
       "      <th>3000</th>\n",
       "      <th>4510</th>\n",
       "      <th>5510</th>\n",
       "      <th>6000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20200101</td>\n",
       "      <td>0</td>\n",
       "      <td>83247</td>\n",
       "      <td>19128</td>\n",
       "      <td>2611</td>\n",
       "      <td>5161</td>\n",
       "      <td>1588</td>\n",
       "      <td>892</td>\n",
       "      <td>32263</td>\n",
       "      <td>1636</td>\n",
       "      <td>...</td>\n",
       "      <td>1311</td>\n",
       "      <td>3482</td>\n",
       "      <td>11299</td>\n",
       "      <td>7072</td>\n",
       "      <td>1176</td>\n",
       "      <td>3810</td>\n",
       "      <td>748</td>\n",
       "      <td>3920</td>\n",
       "      <td>2133</td>\n",
       "      <td>3799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20200101</td>\n",
       "      <td>1</td>\n",
       "      <td>89309</td>\n",
       "      <td>19027</td>\n",
       "      <td>3337</td>\n",
       "      <td>5502</td>\n",
       "      <td>1650</td>\n",
       "      <td>1043</td>\n",
       "      <td>35609</td>\n",
       "      <td>1644</td>\n",
       "      <td>...</td>\n",
       "      <td>1162</td>\n",
       "      <td>3849</td>\n",
       "      <td>13180</td>\n",
       "      <td>8771</td>\n",
       "      <td>1283</td>\n",
       "      <td>3763</td>\n",
       "      <td>782</td>\n",
       "      <td>3483</td>\n",
       "      <td>2057</td>\n",
       "      <td>4010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20200101</td>\n",
       "      <td>2</td>\n",
       "      <td>66611</td>\n",
       "      <td>14710</td>\n",
       "      <td>2970</td>\n",
       "      <td>4631</td>\n",
       "      <td>1044</td>\n",
       "      <td>921</td>\n",
       "      <td>26821</td>\n",
       "      <td>1104</td>\n",
       "      <td>...</td>\n",
       "      <td>768</td>\n",
       "      <td>2299</td>\n",
       "      <td>7986</td>\n",
       "      <td>5426</td>\n",
       "      <td>1536</td>\n",
       "      <td>3229</td>\n",
       "      <td>491</td>\n",
       "      <td>2634</td>\n",
       "      <td>1526</td>\n",
       "      <td>3388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20200101</td>\n",
       "      <td>3</td>\n",
       "      <td>53290</td>\n",
       "      <td>13753</td>\n",
       "      <td>2270</td>\n",
       "      <td>4242</td>\n",
       "      <td>1021</td>\n",
       "      <td>790</td>\n",
       "      <td>21322</td>\n",
       "      <td>909</td>\n",
       "      <td>...</td>\n",
       "      <td>632</td>\n",
       "      <td>1716</td>\n",
       "      <td>5703</td>\n",
       "      <td>3156</td>\n",
       "      <td>1104</td>\n",
       "      <td>2882</td>\n",
       "      <td>431</td>\n",
       "      <td>2488</td>\n",
       "      <td>1268</td>\n",
       "      <td>3686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20200101</td>\n",
       "      <td>4</td>\n",
       "      <td>52095</td>\n",
       "      <td>17615</td>\n",
       "      <td>2406</td>\n",
       "      <td>3689</td>\n",
       "      <td>1840</td>\n",
       "      <td>922</td>\n",
       "      <td>22711</td>\n",
       "      <td>1354</td>\n",
       "      <td>...</td>\n",
       "      <td>875</td>\n",
       "      <td>2421</td>\n",
       "      <td>5816</td>\n",
       "      <td>2933</td>\n",
       "      <td>1206</td>\n",
       "      <td>2433</td>\n",
       "      <td>499</td>\n",
       "      <td>2952</td>\n",
       "      <td>1927</td>\n",
       "      <td>5608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3274</th>\n",
       "      <td>20200517</td>\n",
       "      <td>19</td>\n",
       "      <td>311727</td>\n",
       "      <td>101285</td>\n",
       "      <td>10085</td>\n",
       "      <td>30637</td>\n",
       "      <td>10060</td>\n",
       "      <td>8749</td>\n",
       "      <td>148935</td>\n",
       "      <td>6801</td>\n",
       "      <td>...</td>\n",
       "      <td>6726</td>\n",
       "      <td>15431</td>\n",
       "      <td>25597</td>\n",
       "      <td>14292</td>\n",
       "      <td>9300</td>\n",
       "      <td>22238</td>\n",
       "      <td>3786</td>\n",
       "      <td>16936</td>\n",
       "      <td>10729</td>\n",
       "      <td>20194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3275</th>\n",
       "      <td>20200517</td>\n",
       "      <td>20</td>\n",
       "      <td>305354</td>\n",
       "      <td>91426</td>\n",
       "      <td>8607</td>\n",
       "      <td>26021</td>\n",
       "      <td>8095</td>\n",
       "      <td>7198</td>\n",
       "      <td>136503</td>\n",
       "      <td>6147</td>\n",
       "      <td>...</td>\n",
       "      <td>5501</td>\n",
       "      <td>15378</td>\n",
       "      <td>24661</td>\n",
       "      <td>14747</td>\n",
       "      <td>8239</td>\n",
       "      <td>20604</td>\n",
       "      <td>3203</td>\n",
       "      <td>15018</td>\n",
       "      <td>9767</td>\n",
       "      <td>17962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3276</th>\n",
       "      <td>20200517</td>\n",
       "      <td>21</td>\n",
       "      <td>306008</td>\n",
       "      <td>75113</td>\n",
       "      <td>6325</td>\n",
       "      <td>19933</td>\n",
       "      <td>5711</td>\n",
       "      <td>4494</td>\n",
       "      <td>129412</td>\n",
       "      <td>5134</td>\n",
       "      <td>...</td>\n",
       "      <td>4216</td>\n",
       "      <td>12558</td>\n",
       "      <td>22781</td>\n",
       "      <td>14081</td>\n",
       "      <td>6392</td>\n",
       "      <td>17937</td>\n",
       "      <td>2447</td>\n",
       "      <td>12403</td>\n",
       "      <td>7825</td>\n",
       "      <td>14031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3277</th>\n",
       "      <td>20200517</td>\n",
       "      <td>22</td>\n",
       "      <td>237447</td>\n",
       "      <td>49498</td>\n",
       "      <td>4209</td>\n",
       "      <td>12145</td>\n",
       "      <td>3891</td>\n",
       "      <td>2718</td>\n",
       "      <td>96698</td>\n",
       "      <td>3526</td>\n",
       "      <td>...</td>\n",
       "      <td>2578</td>\n",
       "      <td>8870</td>\n",
       "      <td>16640</td>\n",
       "      <td>11066</td>\n",
       "      <td>4427</td>\n",
       "      <td>11955</td>\n",
       "      <td>1495</td>\n",
       "      <td>7507</td>\n",
       "      <td>5387</td>\n",
       "      <td>8889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3278</th>\n",
       "      <td>20200517</td>\n",
       "      <td>23</td>\n",
       "      <td>150312</td>\n",
       "      <td>27410</td>\n",
       "      <td>2350</td>\n",
       "      <td>6406</td>\n",
       "      <td>1803</td>\n",
       "      <td>1614</td>\n",
       "      <td>55788</td>\n",
       "      <td>1849</td>\n",
       "      <td>...</td>\n",
       "      <td>1377</td>\n",
       "      <td>5021</td>\n",
       "      <td>10058</td>\n",
       "      <td>7139</td>\n",
       "      <td>2250</td>\n",
       "      <td>6844</td>\n",
       "      <td>735</td>\n",
       "      <td>4116</td>\n",
       "      <td>3046</td>\n",
       "      <td>4606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3279 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            날짜  시간      10     100    101    120    121   140     150   160  \\\n",
       "0     20200101   0   83247   19128   2611   5161   1588   892   32263  1636   \n",
       "1     20200101   1   89309   19027   3337   5502   1650  1043   35609  1644   \n",
       "2     20200101   2   66611   14710   2970   4631   1044   921   26821  1104   \n",
       "3     20200101   3   53290   13753   2270   4242   1021   790   21322   909   \n",
       "4     20200101   4   52095   17615   2406   3689   1840   922   22711  1354   \n",
       "...        ...  ..     ...     ...    ...    ...    ...   ...     ...   ...   \n",
       "3274  20200517  19  311727  101285  10085  30637  10060  8749  148935  6801   \n",
       "3275  20200517  20  305354   91426   8607  26021   8095  7198  136503  6147   \n",
       "3276  20200517  21  306008   75113   6325  19933   5711  4494  129412  5134   \n",
       "3277  20200517  22  237447   49498   4209  12145   3891  2718   96698  3526   \n",
       "3278  20200517  23  150312   27410   2350   6406   1803  1614   55788  1849   \n",
       "\n",
       "      ...  1020   1040   1100   1200  1510   2510  3000   4510   5510   6000  \n",
       "0     ...  1311   3482  11299   7072  1176   3810   748   3920   2133   3799  \n",
       "1     ...  1162   3849  13180   8771  1283   3763   782   3483   2057   4010  \n",
       "2     ...   768   2299   7986   5426  1536   3229   491   2634   1526   3388  \n",
       "3     ...   632   1716   5703   3156  1104   2882   431   2488   1268   3686  \n",
       "4     ...   875   2421   5816   2933  1206   2433   499   2952   1927   5608  \n",
       "...   ...   ...    ...    ...    ...   ...    ...   ...    ...    ...    ...  \n",
       "3274  ...  6726  15431  25597  14292  9300  22238  3786  16936  10729  20194  \n",
       "3275  ...  5501  15378  24661  14747  8239  20604  3203  15018   9767  17962  \n",
       "3276  ...  4216  12558  22781  14081  6392  17937  2447  12403   7825  14031  \n",
       "3277  ...  2578   8870  16640  11066  4427  11955  1495   7507   5387   8889  \n",
       "3278  ...  1377   5021  10058   7139  2250   6844   735   4116   3046   4606  \n",
       "\n",
       "[3279 rows x 37 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(os.path.join(DATASET_PATH, 'train.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>날짜</th>\n",
       "      <th>시간</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>140</th>\n",
       "      <th>150</th>\n",
       "      <th>160</th>\n",
       "      <th>...</th>\n",
       "      <th>1020</th>\n",
       "      <th>1040</th>\n",
       "      <th>1100</th>\n",
       "      <th>1200</th>\n",
       "      <th>1510</th>\n",
       "      <th>2510</th>\n",
       "      <th>3000</th>\n",
       "      <th>4510</th>\n",
       "      <th>5510</th>\n",
       "      <th>6000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20200511</td>\n",
       "      <td>0</td>\n",
       "      <td>77968</td>\n",
       "      <td>14429</td>\n",
       "      <td>1233</td>\n",
       "      <td>4021</td>\n",
       "      <td>981</td>\n",
       "      <td>881</td>\n",
       "      <td>28672</td>\n",
       "      <td>1064</td>\n",
       "      <td>...</td>\n",
       "      <td>637</td>\n",
       "      <td>2604</td>\n",
       "      <td>5239</td>\n",
       "      <td>4168</td>\n",
       "      <td>1155</td>\n",
       "      <td>3596</td>\n",
       "      <td>337</td>\n",
       "      <td>2262</td>\n",
       "      <td>1608</td>\n",
       "      <td>2337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20200511</td>\n",
       "      <td>1</td>\n",
       "      <td>48679</td>\n",
       "      <td>9136</td>\n",
       "      <td>823</td>\n",
       "      <td>2618</td>\n",
       "      <td>654</td>\n",
       "      <td>572</td>\n",
       "      <td>17722</td>\n",
       "      <td>672</td>\n",
       "      <td>...</td>\n",
       "      <td>353</td>\n",
       "      <td>1870</td>\n",
       "      <td>3359</td>\n",
       "      <td>2558</td>\n",
       "      <td>1002</td>\n",
       "      <td>2157</td>\n",
       "      <td>257</td>\n",
       "      <td>1425</td>\n",
       "      <td>1018</td>\n",
       "      <td>1810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20200511</td>\n",
       "      <td>2</td>\n",
       "      <td>33773</td>\n",
       "      <td>8199</td>\n",
       "      <td>578</td>\n",
       "      <td>2188</td>\n",
       "      <td>392</td>\n",
       "      <td>502</td>\n",
       "      <td>14464</td>\n",
       "      <td>579</td>\n",
       "      <td>...</td>\n",
       "      <td>345</td>\n",
       "      <td>1499</td>\n",
       "      <td>2646</td>\n",
       "      <td>2022</td>\n",
       "      <td>876</td>\n",
       "      <td>1959</td>\n",
       "      <td>232</td>\n",
       "      <td>1155</td>\n",
       "      <td>927</td>\n",
       "      <td>1530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20200511</td>\n",
       "      <td>3</td>\n",
       "      <td>41511</td>\n",
       "      <td>9986</td>\n",
       "      <td>726</td>\n",
       "      <td>2817</td>\n",
       "      <td>555</td>\n",
       "      <td>646</td>\n",
       "      <td>17793</td>\n",
       "      <td>650</td>\n",
       "      <td>...</td>\n",
       "      <td>390</td>\n",
       "      <td>1730</td>\n",
       "      <td>3398</td>\n",
       "      <td>1967</td>\n",
       "      <td>912</td>\n",
       "      <td>2462</td>\n",
       "      <td>281</td>\n",
       "      <td>1477</td>\n",
       "      <td>959</td>\n",
       "      <td>1882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20200511</td>\n",
       "      <td>4</td>\n",
       "      <td>78680</td>\n",
       "      <td>19509</td>\n",
       "      <td>1463</td>\n",
       "      <td>4720</td>\n",
       "      <td>825</td>\n",
       "      <td>1088</td>\n",
       "      <td>35125</td>\n",
       "      <td>997</td>\n",
       "      <td>...</td>\n",
       "      <td>679</td>\n",
       "      <td>2958</td>\n",
       "      <td>7369</td>\n",
       "      <td>4120</td>\n",
       "      <td>1569</td>\n",
       "      <td>4568</td>\n",
       "      <td>577</td>\n",
       "      <td>3155</td>\n",
       "      <td>1871</td>\n",
       "      <td>3656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>20200524</td>\n",
       "      <td>19</td>\n",
       "      <td>314226</td>\n",
       "      <td>98345</td>\n",
       "      <td>10625</td>\n",
       "      <td>28618</td>\n",
       "      <td>8316</td>\n",
       "      <td>6684</td>\n",
       "      <td>141675</td>\n",
       "      <td>6619</td>\n",
       "      <td>...</td>\n",
       "      <td>8254</td>\n",
       "      <td>16118</td>\n",
       "      <td>23304</td>\n",
       "      <td>14082</td>\n",
       "      <td>8447</td>\n",
       "      <td>21694</td>\n",
       "      <td>2180</td>\n",
       "      <td>15746</td>\n",
       "      <td>10903</td>\n",
       "      <td>21014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>20200524</td>\n",
       "      <td>20</td>\n",
       "      <td>300001</td>\n",
       "      <td>87871</td>\n",
       "      <td>8226</td>\n",
       "      <td>22706</td>\n",
       "      <td>6981</td>\n",
       "      <td>5743</td>\n",
       "      <td>142933</td>\n",
       "      <td>6295</td>\n",
       "      <td>...</td>\n",
       "      <td>5225</td>\n",
       "      <td>15297</td>\n",
       "      <td>21919</td>\n",
       "      <td>14526</td>\n",
       "      <td>7332</td>\n",
       "      <td>19732</td>\n",
       "      <td>1990</td>\n",
       "      <td>14096</td>\n",
       "      <td>10028</td>\n",
       "      <td>17787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>20200524</td>\n",
       "      <td>21</td>\n",
       "      <td>304150</td>\n",
       "      <td>71126</td>\n",
       "      <td>6002</td>\n",
       "      <td>18317</td>\n",
       "      <td>4939</td>\n",
       "      <td>3779</td>\n",
       "      <td>133110</td>\n",
       "      <td>4781</td>\n",
       "      <td>...</td>\n",
       "      <td>4072</td>\n",
       "      <td>12685</td>\n",
       "      <td>21135</td>\n",
       "      <td>14403</td>\n",
       "      <td>5443</td>\n",
       "      <td>16967</td>\n",
       "      <td>1359</td>\n",
       "      <td>11670</td>\n",
       "      <td>7963</td>\n",
       "      <td>14041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>20200524</td>\n",
       "      <td>22</td>\n",
       "      <td>236751</td>\n",
       "      <td>44947</td>\n",
       "      <td>3575</td>\n",
       "      <td>11455</td>\n",
       "      <td>3135</td>\n",
       "      <td>2536</td>\n",
       "      <td>98582</td>\n",
       "      <td>3267</td>\n",
       "      <td>...</td>\n",
       "      <td>2489</td>\n",
       "      <td>8093</td>\n",
       "      <td>14427</td>\n",
       "      <td>10914</td>\n",
       "      <td>3861</td>\n",
       "      <td>11397</td>\n",
       "      <td>859</td>\n",
       "      <td>7270</td>\n",
       "      <td>5194</td>\n",
       "      <td>8230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>20200524</td>\n",
       "      <td>23</td>\n",
       "      <td>143609</td>\n",
       "      <td>26137</td>\n",
       "      <td>2242</td>\n",
       "      <td>6166</td>\n",
       "      <td>1609</td>\n",
       "      <td>1391</td>\n",
       "      <td>54633</td>\n",
       "      <td>1899</td>\n",
       "      <td>...</td>\n",
       "      <td>1343</td>\n",
       "      <td>4686</td>\n",
       "      <td>8732</td>\n",
       "      <td>6986</td>\n",
       "      <td>2161</td>\n",
       "      <td>6487</td>\n",
       "      <td>410</td>\n",
       "      <td>3963</td>\n",
       "      <td>2686</td>\n",
       "      <td>4690</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>336 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           날짜  시간      10    100    101    120   121   140     150   160  ...  \\\n",
       "0    20200511   0   77968  14429   1233   4021   981   881   28672  1064  ...   \n",
       "1    20200511   1   48679   9136    823   2618   654   572   17722   672  ...   \n",
       "2    20200511   2   33773   8199    578   2188   392   502   14464   579  ...   \n",
       "3    20200511   3   41511   9986    726   2817   555   646   17793   650  ...   \n",
       "4    20200511   4   78680  19509   1463   4720   825  1088   35125   997  ...   \n",
       "..        ...  ..     ...    ...    ...    ...   ...   ...     ...   ...  ...   \n",
       "331  20200524  19  314226  98345  10625  28618  8316  6684  141675  6619  ...   \n",
       "332  20200524  20  300001  87871   8226  22706  6981  5743  142933  6295  ...   \n",
       "333  20200524  21  304150  71126   6002  18317  4939  3779  133110  4781  ...   \n",
       "334  20200524  22  236751  44947   3575  11455  3135  2536   98582  3267  ...   \n",
       "335  20200524  23  143609  26137   2242   6166  1609  1391   54633  1899  ...   \n",
       "\n",
       "     1020   1040   1100   1200  1510   2510  3000   4510   5510   6000  \n",
       "0     637   2604   5239   4168  1155   3596   337   2262   1608   2337  \n",
       "1     353   1870   3359   2558  1002   2157   257   1425   1018   1810  \n",
       "2     345   1499   2646   2022   876   1959   232   1155    927   1530  \n",
       "3     390   1730   3398   1967   912   2462   281   1477    959   1882  \n",
       "4     679   2958   7369   4120  1569   4568   577   3155   1871   3656  \n",
       "..    ...    ...    ...    ...   ...    ...   ...    ...    ...    ...  \n",
       "331  8254  16118  23304  14082  8447  21694  2180  15746  10903  21014  \n",
       "332  5225  15297  21919  14526  7332  19732  1990  14096  10028  17787  \n",
       "333  4072  12685  21135  14403  5443  16967  1359  11670   7963  14041  \n",
       "334  2489   8093  14427  10914  3861  11397   859   7270   5194   8230  \n",
       "335  1343   4686   8732   6986  2161   6487   410   3963   2686   4690  \n",
       "\n",
       "[336 rows x 37 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(os.path.join(DATASET_PATH, 'validate.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>날짜</th>\n",
       "      <th>시간</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>140</th>\n",
       "      <th>150</th>\n",
       "      <th>160</th>\n",
       "      <th>...</th>\n",
       "      <th>1020</th>\n",
       "      <th>1040</th>\n",
       "      <th>1100</th>\n",
       "      <th>1200</th>\n",
       "      <th>1510</th>\n",
       "      <th>2510</th>\n",
       "      <th>3000</th>\n",
       "      <th>4510</th>\n",
       "      <th>5510</th>\n",
       "      <th>6000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20200518</td>\n",
       "      <td>0</td>\n",
       "      <td>82065</td>\n",
       "      <td>15172</td>\n",
       "      <td>1500</td>\n",
       "      <td>3294</td>\n",
       "      <td>1086</td>\n",
       "      <td>962</td>\n",
       "      <td>28931</td>\n",
       "      <td>1103</td>\n",
       "      <td>...</td>\n",
       "      <td>618</td>\n",
       "      <td>2790</td>\n",
       "      <td>5147</td>\n",
       "      <td>4331</td>\n",
       "      <td>1329</td>\n",
       "      <td>3665</td>\n",
       "      <td>404</td>\n",
       "      <td>2242</td>\n",
       "      <td>1619</td>\n",
       "      <td>2314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20200518</td>\n",
       "      <td>1</td>\n",
       "      <td>51248</td>\n",
       "      <td>9840</td>\n",
       "      <td>813</td>\n",
       "      <td>2356</td>\n",
       "      <td>696</td>\n",
       "      <td>546</td>\n",
       "      <td>17888</td>\n",
       "      <td>720</td>\n",
       "      <td>...</td>\n",
       "      <td>430</td>\n",
       "      <td>1864</td>\n",
       "      <td>3269</td>\n",
       "      <td>2561</td>\n",
       "      <td>921</td>\n",
       "      <td>2081</td>\n",
       "      <td>272</td>\n",
       "      <td>1390</td>\n",
       "      <td>1003</td>\n",
       "      <td>1766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20200518</td>\n",
       "      <td>2</td>\n",
       "      <td>39026</td>\n",
       "      <td>7894</td>\n",
       "      <td>760</td>\n",
       "      <td>2413</td>\n",
       "      <td>408</td>\n",
       "      <td>549</td>\n",
       "      <td>13357</td>\n",
       "      <td>498</td>\n",
       "      <td>...</td>\n",
       "      <td>322</td>\n",
       "      <td>1313</td>\n",
       "      <td>2765</td>\n",
       "      <td>1931</td>\n",
       "      <td>920</td>\n",
       "      <td>1764</td>\n",
       "      <td>228</td>\n",
       "      <td>1136</td>\n",
       "      <td>922</td>\n",
       "      <td>1309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20200518</td>\n",
       "      <td>3</td>\n",
       "      <td>40993</td>\n",
       "      <td>10137</td>\n",
       "      <td>780</td>\n",
       "      <td>2701</td>\n",
       "      <td>420</td>\n",
       "      <td>741</td>\n",
       "      <td>15544</td>\n",
       "      <td>532</td>\n",
       "      <td>...</td>\n",
       "      <td>326</td>\n",
       "      <td>1766</td>\n",
       "      <td>3320</td>\n",
       "      <td>2060</td>\n",
       "      <td>892</td>\n",
       "      <td>2447</td>\n",
       "      <td>337</td>\n",
       "      <td>1495</td>\n",
       "      <td>975</td>\n",
       "      <td>1912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20200518</td>\n",
       "      <td>4</td>\n",
       "      <td>77863</td>\n",
       "      <td>19603</td>\n",
       "      <td>1276</td>\n",
       "      <td>5019</td>\n",
       "      <td>968</td>\n",
       "      <td>1160</td>\n",
       "      <td>32101</td>\n",
       "      <td>968</td>\n",
       "      <td>...</td>\n",
       "      <td>669</td>\n",
       "      <td>2914</td>\n",
       "      <td>6986</td>\n",
       "      <td>3911</td>\n",
       "      <td>1368</td>\n",
       "      <td>4380</td>\n",
       "      <td>513</td>\n",
       "      <td>2940</td>\n",
       "      <td>1758</td>\n",
       "      <td>3629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>20200531</td>\n",
       "      <td>19</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>...</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>20200531</td>\n",
       "      <td>20</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>...</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>20200531</td>\n",
       "      <td>21</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>...</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>20200531</td>\n",
       "      <td>22</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>...</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>20200531</td>\n",
       "      <td>23</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>...</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>336 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           날짜  시간     10    100   101   120   121   140    150   160  ...  \\\n",
       "0    20200518   0  82065  15172  1500  3294  1086   962  28931  1103  ...   \n",
       "1    20200518   1  51248   9840   813  2356   696   546  17888   720  ...   \n",
       "2    20200518   2  39026   7894   760  2413   408   549  13357   498  ...   \n",
       "3    20200518   3  40993  10137   780  2701   420   741  15544   532  ...   \n",
       "4    20200518   4  77863  19603  1276  5019   968  1160  32101   968  ...   \n",
       "..        ...  ..    ...    ...   ...   ...   ...   ...    ...   ...  ...   \n",
       "331  20200531  19   -999   -999  -999  -999  -999  -999   -999  -999  ...   \n",
       "332  20200531  20   -999   -999  -999  -999  -999  -999   -999  -999  ...   \n",
       "333  20200531  21   -999   -999  -999  -999  -999  -999   -999  -999  ...   \n",
       "334  20200531  22   -999   -999  -999  -999  -999  -999   -999  -999  ...   \n",
       "335  20200531  23   -999   -999  -999  -999  -999  -999   -999  -999  ...   \n",
       "\n",
       "     1020  1040  1100  1200  1510  2510  3000  4510  5510  6000  \n",
       "0     618  2790  5147  4331  1329  3665   404  2242  1619  2314  \n",
       "1     430  1864  3269  2561   921  2081   272  1390  1003  1766  \n",
       "2     322  1313  2765  1931   920  1764   228  1136   922  1309  \n",
       "3     326  1766  3320  2060   892  2447   337  1495   975  1912  \n",
       "4     669  2914  6986  3911  1368  4380   513  2940  1758  3629  \n",
       "..    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "331  -999  -999  -999  -999  -999  -999  -999  -999  -999  -999  \n",
       "332  -999  -999  -999  -999  -999  -999  -999  -999  -999  -999  \n",
       "333  -999  -999  -999  -999  -999  -999  -999  -999  -999  -999  \n",
       "334  -999  -999  -999  -999  -999  -999  -999  -999  -999  -999  \n",
       "335  -999  -999  -999  -999  -999  -999  -999  -999  -999  -999  \n",
       "\n",
       "[336 rows x 37 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(os.path.join(DATASET_PATH, 'test.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader\n",
    "* 한 칼럼에 대한 7일(168행) 데이터를 input_data, 뒤따르는 7일 데이터를 output_data로 반환합니다.\n",
    "* 도로별 차이를 두지 않고 모든 도로를 동일한 타입의 데이터로 취급합니다.\n",
    "* 모든 csv 파일의 마지막 168행은 예측해야하는 값이므로 input으로 들어가지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "\n",
    "\n",
    "class CustomDataset(data.Dataset):\n",
    "    def __init__(self, root, seq_len, batch_size=64, phase='train'):\n",
    "        self.root = root\n",
    "        self.phase = phase\n",
    "        self.seq_len = seq_len * 24\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = {}\n",
    "\n",
    "        self.label_path = os.path.join(self.root, self.phase + '.csv')\n",
    "\n",
    "        df = pd.read_csv(self.label_path)\n",
    "        timestamps = [(i, j) for (i, j) in zip(list(df['날짜']), list(df['시간']))]\n",
    "        categories = df.columns.values.tolist()[2:]\n",
    "\n",
    "        input_data = []\n",
    "        output_data = []\n",
    "\n",
    "        for t in range(len(timestamps)):\n",
    "            temp_input_data = []\n",
    "            temp_output_data = []\n",
    "            for col in categories:\n",
    "                road = df[col].tolist()\n",
    "                inp = [float(i) for i in road[t:t+self.seq_len]]\n",
    "                outp = [float(j) for j in road[t+self.seq_len:t+2*self.seq_len]]\n",
    "                temp_input_data.append(inp)\n",
    "                temp_output_data.append(outp)\n",
    "            input_data.append(temp_input_data) # input, output 리스트를 다음과 같이 조건 형식에 맞춰서 만들어준다 하는게 key point \n",
    "            output_data.append(temp_output_data)\n",
    "\n",
    "        self.labels['timestamp'] = timestamps\n",
    "        self.labels['category'] = categories\n",
    "        self.labels['input'] = input_data\n",
    "        self.labels['output'] = output_data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        row = index // 35\n",
    "        col = index % 35\n",
    "\n",
    "        timestamp = self.labels['timestamp'][row]\n",
    "        category = self.labels['category'][col]\n",
    "        \n",
    "        input_data = torch.tensor(self.labels['input'][row][col])\n",
    "\n",
    "        if self.phase != 'test':\n",
    "            output_data = torch.tensor(self.labels['output'][row][col])\n",
    "        else:\n",
    "            output_data = []\n",
    "\n",
    "        return timestamp, category, (input_data, output_data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels['timestamp']) - (self.seq_len * 2) + 1) * 35\n",
    "\n",
    "    def get_label_file(self):\n",
    "        return self.label_path\n",
    "\n",
    "\n",
    "def data_loader(root, phase='train', batch_size=64, seq_len=7, drop_last=False):\n",
    "    if phase == 'train':\n",
    "        shuffle = True\n",
    "    else:\n",
    "        shuffle = False\n",
    "\n",
    "    dataset = CustomDataset(root, seq_len, batch_size, phase)\n",
    "    dataloader = data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    return dataloader, dataset.get_label_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_size=168,\n",
    "                 hidden_size=1024,\n",
    "                 output_size=168,\n",
    "\n",
    "                 batch_size=64,\n",
    "\n",
    "                 num_layers=3,\n",
    "                 dropout=0,\n",
    "                 batch_first=False):\n",
    "        super(LSTMNet, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        ##### Layer 1\n",
    "        self.lstm1 = nn.LSTM(input_size,\n",
    "                            hidden_size,\n",
    "                            dropout=0.2,\n",
    "                            num_layers=num_layers)\n",
    "\n",
    "        ##### Layer 2\n",
    "        self.lstm2 = nn.LSTM(hidden_size, \n",
    "                             hidden_size,\n",
    "                             dropout=0.2,\n",
    "                             num_layers=num_layers)\n",
    "\n",
    "        ##### Finalize\n",
    "        self.linear = nn.Linear(hidden_size, \n",
    "                                output_size)\n",
    "        \n",
    "        self.activation = nn.LeakyReLU(0.2)\n",
    "\n",
    "\n",
    "    def forward(self, x, h_in, c_in):\n",
    "\n",
    "        h_in = nn.Parameter(h_in.type(dtype), requires_grad=True)\n",
    "        c_in = nn.Parameter(c_in.type(dtype), requires_grad=True)\n",
    "\n",
    "        # Layer 1\n",
    "        lstm_out, (h_1, c_1) = self.lstm1(x, (h_in, c_in))\n",
    "        lstm_out = self.activation(lstm_out)\n",
    "\n",
    "        # Layer2\n",
    "        lstm_out, (h_2, c_2) = self.lstm2(lstm_out, (h_1, c_1))\n",
    "        lstm_out = self.activation(lstm_out)\n",
    "\n",
    "        # 이렇게 레이어를 쌓아주고 linear 해준다. \n",
    "        \n",
    "        # Final\n",
    "        predictions = self.linear(lstm_out)\n",
    "        \n",
    "        return predictions, (h_2, c_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSLELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, pred, actual):\n",
    "        return torch.sqrt(self.mse(torch.log(pred + 1), torch.log(actual + 1)))\n",
    "\n",
    "\n",
    "def save_model(model_name, model, optimizer, scheduler):\n",
    "    state = {\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'scheduler': scheduler.state_dict()\n",
    "    }\n",
    "    torch.save(state, os.path.join('log', model_name + '.pth'))\n",
    "    print('model saved')\n",
    "    return os.path.join('log', model_name + '.pth')\n",
    "\n",
    "\n",
    "def load_model(model_name, model, optimizer=None, scheduler=None):\n",
    "    state = torch.load(os.path.join(model_name))\n",
    "    model.load_state_dict(state['model'])\n",
    "    if optimizer is not None:\n",
    "        optimizer.load_state_dict(state['optimizer'])\n",
    "    if scheduler is not None:\n",
    "        scheduler.load_state_dict(state['scheduler'])\n",
    "    print('model loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼 파라미터를 다음과 같이 셋팅하고 돌린다. 하는 것 \n",
    "dtype = torch.float\n",
    "model_name = 'sequential'\n",
    "\n",
    "batch_size = 64\n",
    "num_epochs = 20\n",
    "print_iter = 200\n",
    "val_epoch = 1    # epoch은 우리가 조정해서 성능을 높여보자. \n",
    "save_epoch = 1\n",
    "base_lr = 0.01\n",
    "seq_len = 7 # 날짜를 이렇게 7로 지정\n",
    "\n",
    "input_size = seq_len * 24\n",
    "output_size = input_size\n",
    "hidden_size = 1024\n",
    "num_layers = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결과 파일과 모델 가중치 파일 저장을 위해 log 디렉토리가 생성됩니다. 중요한 파일이 덮어씌워지지 않도록 주의 바랍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('log', exist_ok=True) #log라는 폴더를 만들어 주는 코드임 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model = LSTMNet(input_size=input_size,\n",
    "                hidden_size=hidden_size,\n",
    "                output_size=output_size,\n",
    "                batch_size=batch_size,\n",
    "                num_layers=num_layers)\n",
    "model = model.to(device)\n",
    "\n",
    "# loss function\n",
    "criterion = RMSLELoss()\n",
    "\n",
    "# optimizer & scheduler\n",
    "optimizer = Adam(model.parameters(), lr=base_lr)\n",
    "scheduler = StepLR(optimizer, step_size=20, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMNet(\n",
      "  (lstm1): LSTM(168, 1024, num_layers=6, dropout=0.2)\n",
      "  (lstm2): LSTM(1024, 1024, num_layers=6, dropout=0.2)\n",
      "  (linear): Linear(in_features=1024, out_features=168, bias=True)\n",
      "  (activation): LeakyReLU(negative_slope=0.2)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model) # 우리가 만들었던 대로 layer 두개가 겹친 모델이 나올 것. 출력해봐. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "num of parameters :  97427624\n",
      "num of trainable parameters : 97427624\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# get data loader\n",
    "train_dataloader, _ = data_loader(root=DATASET_PATH,\n",
    "                                  phase='train',\n",
    "                                  batch_size=batch_size,\n",
    "                                  seq_len=seq_len,\n",
    "                                  drop_last=True)\n",
    "\n",
    "validate_dataloader, _ = data_loader(root=DATASET_PATH,\n",
    "                                     phase='validate',\n",
    "                                     batch_size=1,\n",
    "                                     seq_len=seq_len,\n",
    "                                     drop_last=True)\n",
    "\n",
    "print(\"------------------------------------------------------------\")\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"num of parameters : \",total_params)\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"num of trainable parameters :\", trainable_params)\n",
    "print(\"------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:     0 | Iteration:     0 | Loss: 9.68072\n",
      "Epoch:     0 | Iteration:   200 | Loss: 4.06578\n",
      "Epoch:     0 | Iteration:   400 | Loss: 3.69508\n",
      "Epoch:     0 | Iteration:   600 | Loss: 3.47032\n",
      "Epoch:     0 | Iteration:   800 | Loss: 3.30369\n",
      "Epoch:     0 | Iteration:  1000 | Loss: 2.88878\n",
      "Epoch:     0 | Iteration:  1200 | Loss: 2.88188\n",
      "Epoch:     0 | Iteration:  1400 | Loss: 3.05354\n",
      "Epoch:     0 | Iteration:  1600 | Loss: 2.77211\n",
      "\n",
      "Epoch:     0 | Loss: 3.48003\n",
      "\n",
      "model saved\n",
      "best model so far: epoch 0\n",
      "model saved\n",
      "last model: epoch 0\n",
      "\n",
      "Validation Epoch:     0 | Loss: 2.67506\n",
      "\n",
      "Epoch:     1 | Iteration:     0 | Loss: 2.92652\n",
      "Epoch:     1 | Iteration:   200 | Loss: 3.05159\n",
      "Epoch:     1 | Iteration:   400 | Loss: 2.70383\n",
      "Epoch:     1 | Iteration:   600 | Loss: 2.35922\n",
      "Epoch:     1 | Iteration:   800 | Loss: 2.48133\n",
      "Epoch:     1 | Iteration:  1000 | Loss: 2.42557\n",
      "Epoch:     1 | Iteration:  1200 | Loss: 2.07236\n",
      "Epoch:     1 | Iteration:  1400 | Loss: 2.22088\n",
      "Epoch:     1 | Iteration:  1600 | Loss: 2.43235\n",
      "\n",
      "Epoch:     1 | Loss: 2.50830\n",
      "\n",
      "model saved\n",
      "last model: epoch 1\n",
      "\n",
      "Validation Epoch:     1 | Loss: 2.14988\n",
      "\n",
      "Epoch:     2 | Iteration:     0 | Loss: 2.35961\n",
      "Epoch:     2 | Iteration:   200 | Loss: 2.24702\n",
      "Epoch:     2 | Iteration:   400 | Loss: 2.30255\n",
      "Epoch:     2 | Iteration:   600 | Loss: 2.01787\n",
      "Epoch:     2 | Iteration:   800 | Loss: 2.12556\n",
      "Epoch:     2 | Iteration:  1000 | Loss: 1.97391\n",
      "Epoch:     2 | Iteration:  1200 | Loss: 1.87030\n",
      "Epoch:     2 | Iteration:  1400 | Loss: 1.85959\n",
      "Epoch:     2 | Iteration:  1600 | Loss: 1.88732\n",
      "\n",
      "Epoch:     2 | Loss: 2.10545\n",
      "\n",
      "model saved\n",
      "last model: epoch 2\n",
      "\n",
      "Validation Epoch:     2 | Loss: 1.83683\n",
      "\n",
      "Epoch:     3 | Iteration:     0 | Loss: 1.93424\n",
      "Epoch:     3 | Iteration:   200 | Loss: 1.90378\n",
      "Epoch:     3 | Iteration:   400 | Loss: 1.84347\n",
      "Epoch:     3 | Iteration:   600 | Loss: 1.92278\n",
      "Epoch:     3 | Iteration:   800 | Loss: 1.76624\n",
      "Epoch:     3 | Iteration:  1000 | Loss: 1.89271\n",
      "Epoch:     3 | Iteration:  1200 | Loss: 1.72535\n",
      "Epoch:     3 | Iteration:  1400 | Loss: 1.92243\n",
      "Epoch:     3 | Iteration:  1600 | Loss: 1.78530\n",
      "\n",
      "Epoch:     3 | Loss: 1.84101\n",
      "\n",
      "model saved\n",
      "last model: epoch 3\n",
      "\n",
      "Validation Epoch:     3 | Loss: 1.63165\n",
      "\n",
      "Epoch:     4 | Iteration:     0 | Loss: 1.75373\n",
      "Epoch:     4 | Iteration:   200 | Loss: 1.76903\n",
      "Epoch:     4 | Iteration:   400 | Loss: 1.84479\n",
      "Epoch:     4 | Iteration:   600 | Loss: 1.60733\n",
      "Epoch:     4 | Iteration:   800 | Loss: 1.83362\n",
      "Epoch:     4 | Iteration:  1000 | Loss: 1.70070\n",
      "Epoch:     4 | Iteration:  1200 | Loss: 1.62953\n",
      "Epoch:     4 | Iteration:  1400 | Loss: 1.70424\n",
      "Epoch:     4 | Iteration:  1600 | Loss: 1.53671\n",
      "\n",
      "Epoch:     4 | Loss: 1.66274\n",
      "\n",
      "model saved\n",
      "last model: epoch 4\n",
      "\n",
      "Validation Epoch:     4 | Loss: 1.50322\n",
      "\n",
      "Epoch:     5 | Iteration:     0 | Loss: 1.55381\n",
      "Epoch:     5 | Iteration:   200 | Loss: 1.55828\n",
      "Epoch:     5 | Iteration:   400 | Loss: 1.56063\n",
      "Epoch:     5 | Iteration:   600 | Loss: 1.64958\n",
      "Epoch:     5 | Iteration:   800 | Loss: 1.41863\n",
      "Epoch:     5 | Iteration:  1000 | Loss: 1.44531\n",
      "Epoch:     5 | Iteration:  1200 | Loss: 1.65547\n",
      "Epoch:     5 | Iteration:  1400 | Loss: 1.45229\n",
      "Epoch:     5 | Iteration:  1600 | Loss: 1.56854\n",
      "\n",
      "Epoch:     5 | Loss: 1.55262\n",
      "\n",
      "model saved\n",
      "last model: epoch 5\n",
      "\n",
      "Validation Epoch:     5 | Loss: 1.43202\n",
      "\n",
      "Epoch:     6 | Iteration:     0 | Loss: 1.50255\n",
      "Epoch:     6 | Iteration:   200 | Loss: 1.54976\n",
      "Epoch:     6 | Iteration:   400 | Loss: 1.58574\n",
      "Epoch:     6 | Iteration:   600 | Loss: 1.50834\n",
      "Epoch:     6 | Iteration:   800 | Loss: 1.67644\n",
      "Epoch:     6 | Iteration:  1000 | Loss: 1.40596\n",
      "Epoch:     6 | Iteration:  1200 | Loss: 1.49538\n",
      "Epoch:     6 | Iteration:  1400 | Loss: 1.61379\n",
      "Epoch:     6 | Iteration:  1600 | Loss: 1.28651\n",
      "\n",
      "Epoch:     6 | Loss: 1.49522\n",
      "\n",
      "model saved\n",
      "last model: epoch 6\n",
      "\n",
      "Validation Epoch:     6 | Loss: 1.40038\n",
      "\n",
      "Epoch:     7 | Iteration:     0 | Loss: 1.50907\n",
      "Epoch:     7 | Iteration:   200 | Loss: 1.58103\n",
      "Epoch:     7 | Iteration:   400 | Loss: 1.42905\n",
      "Epoch:     7 | Iteration:   600 | Loss: 1.51523\n",
      "Epoch:     7 | Iteration:   800 | Loss: 1.31191\n",
      "Epoch:     7 | Iteration:  1000 | Loss: 1.41547\n",
      "Epoch:     7 | Iteration:  1200 | Loss: 1.50500\n",
      "Epoch:     7 | Iteration:  1400 | Loss: 1.45718\n",
      "Epoch:     7 | Iteration:  1600 | Loss: 1.42031\n",
      "\n",
      "Epoch:     7 | Loss: 1.47183\n",
      "\n",
      "model saved\n",
      "last model: epoch 7\n",
      "\n",
      "Validation Epoch:     7 | Loss: 1.38980\n",
      "\n",
      "Epoch:     8 | Iteration:     0 | Loss: 1.40632\n",
      "Epoch:     8 | Iteration:   200 | Loss: 1.56865\n",
      "Epoch:     8 | Iteration:   400 | Loss: 1.50657\n",
      "Epoch:     8 | Iteration:   600 | Loss: 1.43968\n",
      "Epoch:     8 | Iteration:   800 | Loss: 1.47058\n",
      "Epoch:     8 | Iteration:  1000 | Loss: 1.44795\n",
      "Epoch:     8 | Iteration:  1200 | Loss: 1.41417\n",
      "Epoch:     8 | Iteration:  1400 | Loss: 1.46152\n",
      "Epoch:     8 | Iteration:  1600 | Loss: 1.49282\n",
      "\n",
      "Epoch:     8 | Loss: 1.46566\n",
      "\n",
      "model saved\n",
      "last model: epoch 8\n",
      "\n",
      "Validation Epoch:     8 | Loss: 1.38869\n",
      "\n",
      "Epoch:     9 | Iteration:     0 | Loss: 1.54464\n",
      "Epoch:     9 | Iteration:   200 | Loss: 1.45548\n",
      "Epoch:     9 | Iteration:   400 | Loss: 1.53478\n",
      "Epoch:     9 | Iteration:   600 | Loss: 1.34299\n",
      "Epoch:     9 | Iteration:   800 | Loss: 1.35453\n",
      "Epoch:     9 | Iteration:  1000 | Loss: 1.55384\n",
      "Epoch:     9 | Iteration:  1200 | Loss: 1.56943\n",
      "Epoch:     9 | Iteration:  1400 | Loss: 1.48566\n",
      "Epoch:     9 | Iteration:  1600 | Loss: 1.64416\n",
      "\n",
      "Epoch:     9 | Loss: 1.46480\n",
      "\n",
      "model saved\n",
      "last model: epoch 9\n",
      "\n",
      "Validation Epoch:     9 | Loss: 1.38789\n",
      "\n",
      "Epoch:    10 | Iteration:     0 | Loss: 1.54707\n",
      "Epoch:    10 | Iteration:   200 | Loss: 1.49998\n",
      "Epoch:    10 | Iteration:   400 | Loss: 1.45516\n",
      "Epoch:    10 | Iteration:   600 | Loss: 1.39437\n",
      "Epoch:    10 | Iteration:   800 | Loss: 1.41782\n",
      "Epoch:    10 | Iteration:  1000 | Loss: 1.48453\n",
      "Epoch:    10 | Iteration:  1200 | Loss: 1.41267\n",
      "Epoch:    10 | Iteration:  1400 | Loss: 1.46676\n",
      "Epoch:    10 | Iteration:  1600 | Loss: 1.53899\n",
      "\n",
      "Epoch:    10 | Loss: 1.46478\n",
      "\n",
      "model saved\n",
      "last model: epoch 10\n",
      "\n",
      "Validation Epoch:    10 | Loss: 1.38824\n",
      "\n",
      "Epoch:    11 | Iteration:     0 | Loss: 1.38553\n",
      "Epoch:    11 | Iteration:   200 | Loss: 1.50573\n",
      "Epoch:    11 | Iteration:   400 | Loss: 1.47437\n",
      "Epoch:    11 | Iteration:   600 | Loss: 1.45479\n",
      "Epoch:    11 | Iteration:   800 | Loss: 1.39788\n",
      "Epoch:    11 | Iteration:  1000 | Loss: 1.32686\n",
      "Epoch:    11 | Iteration:  1200 | Loss: 1.42437\n",
      "Epoch:    11 | Iteration:  1400 | Loss: 1.56867\n",
      "Epoch:    11 | Iteration:  1600 | Loss: 1.42907\n",
      "\n",
      "Epoch:    11 | Loss: 1.46490\n",
      "\n",
      "model saved\n",
      "last model: epoch 11\n",
      "\n",
      "Validation Epoch:    11 | Loss: 1.38850\n",
      "\n",
      "Epoch:    12 | Iteration:     0 | Loss: 1.40551\n",
      "Epoch:    12 | Iteration:   200 | Loss: 1.50611\n",
      "Epoch:    12 | Iteration:   400 | Loss: 1.36534\n",
      "Epoch:    12 | Iteration:   600 | Loss: 1.41773\n",
      "Epoch:    12 | Iteration:   800 | Loss: 1.46189\n",
      "Epoch:    12 | Iteration:  1000 | Loss: 1.45448\n",
      "Epoch:    12 | Iteration:  1200 | Loss: 1.37744\n",
      "Epoch:    12 | Iteration:  1400 | Loss: 1.45374\n",
      "Epoch:    12 | Iteration:  1600 | Loss: 1.49039\n",
      "\n",
      "Epoch:    12 | Loss: 1.46483\n",
      "\n",
      "model saved\n",
      "last model: epoch 12\n",
      "\n",
      "Validation Epoch:    12 | Loss: 1.38788\n",
      "\n",
      "Epoch:    13 | Iteration:     0 | Loss: 1.37398\n",
      "Epoch:    13 | Iteration:   200 | Loss: 1.43077\n",
      "Epoch:    13 | Iteration:   400 | Loss: 1.43008\n",
      "Epoch:    13 | Iteration:   600 | Loss: 1.42386\n",
      "Epoch:    13 | Iteration:   800 | Loss: 1.45848\n",
      "Epoch:    13 | Iteration:  1000 | Loss: 1.30357\n",
      "Epoch:    13 | Iteration:  1200 | Loss: 1.50504\n",
      "Epoch:    13 | Iteration:  1400 | Loss: 1.57202\n",
      "Epoch:    13 | Iteration:  1600 | Loss: 1.62648\n",
      "\n",
      "Epoch:    13 | Loss: 1.46470\n",
      "\n",
      "model saved\n",
      "last model: epoch 13\n",
      "\n",
      "Validation Epoch:    13 | Loss: 1.38740\n",
      "\n",
      "Epoch:    14 | Iteration:     0 | Loss: 1.30729\n",
      "Epoch:    14 | Iteration:   200 | Loss: 1.49906\n",
      "Epoch:    14 | Iteration:   400 | Loss: 1.45928\n",
      "Epoch:    14 | Iteration:   600 | Loss: 1.40084\n",
      "Epoch:    14 | Iteration:   800 | Loss: 1.53767\n",
      "Epoch:    14 | Iteration:  1000 | Loss: 1.48307\n",
      "Epoch:    14 | Iteration:  1200 | Loss: 1.63691\n",
      "Epoch:    14 | Iteration:  1400 | Loss: 1.49263\n",
      "Epoch:    14 | Iteration:  1600 | Loss: 1.49488\n",
      "\n",
      "Epoch:    14 | Loss: 1.46477\n",
      "\n",
      "model saved\n",
      "last model: epoch 14\n",
      "\n",
      "Validation Epoch:    14 | Loss: 1.38707\n",
      "\n",
      "Epoch:    15 | Iteration:     0 | Loss: 1.55106\n",
      "Epoch:    15 | Iteration:   200 | Loss: 1.46981\n",
      "Epoch:    15 | Iteration:   400 | Loss: 1.40280\n",
      "Epoch:    15 | Iteration:   600 | Loss: 1.42676\n",
      "Epoch:    15 | Iteration:   800 | Loss: 1.40328\n",
      "Epoch:    15 | Iteration:  1000 | Loss: 1.52278\n",
      "Epoch:    15 | Iteration:  1200 | Loss: 1.42566\n",
      "Epoch:    15 | Iteration:  1400 | Loss: 1.35221\n",
      "Epoch:    15 | Iteration:  1600 | Loss: 1.46408\n",
      "\n",
      "Epoch:    15 | Loss: 1.46476\n",
      "\n",
      "model saved\n",
      "last model: epoch 15\n",
      "\n",
      "Validation Epoch:    15 | Loss: 1.38775\n",
      "\n",
      "Epoch:    16 | Iteration:     0 | Loss: 1.42726\n",
      "Epoch:    16 | Iteration:   200 | Loss: 1.45792\n",
      "Epoch:    16 | Iteration:   400 | Loss: 1.55402\n",
      "Epoch:    16 | Iteration:   600 | Loss: 1.36443\n",
      "Epoch:    16 | Iteration:   800 | Loss: 1.41772\n",
      "Epoch:    16 | Iteration:  1000 | Loss: 1.42111\n",
      "Epoch:    16 | Iteration:  1200 | Loss: 1.50675\n",
      "Epoch:    16 | Iteration:  1400 | Loss: 1.54208\n",
      "Epoch:    16 | Iteration:  1600 | Loss: 1.42493\n",
      "\n",
      "Epoch:    16 | Loss: 1.46486\n",
      "\n",
      "model saved\n",
      "last model: epoch 16\n",
      "\n",
      "Validation Epoch:    16 | Loss: 1.38765\n",
      "\n",
      "Epoch:    17 | Iteration:     0 | Loss: 1.39640\n",
      "Epoch:    17 | Iteration:   200 | Loss: 1.37461\n",
      "Epoch:    17 | Iteration:   400 | Loss: 1.53428\n",
      "Epoch:    17 | Iteration:   600 | Loss: 1.57340\n",
      "Epoch:    17 | Iteration:   800 | Loss: 1.50923\n",
      "Epoch:    17 | Iteration:  1000 | Loss: 1.36464\n",
      "Epoch:    17 | Iteration:  1200 | Loss: 1.48845\n",
      "Epoch:    17 | Iteration:  1400 | Loss: 1.46665\n",
      "Epoch:    17 | Iteration:  1600 | Loss: 1.41644\n",
      "\n",
      "Epoch:    17 | Loss: 1.46482\n",
      "\n",
      "model saved\n",
      "last model: epoch 17\n",
      "\n",
      "Validation Epoch:    17 | Loss: 1.38796\n",
      "\n",
      "Epoch:    18 | Iteration:     0 | Loss: 1.40567\n",
      "Epoch:    18 | Iteration:   200 | Loss: 1.31343\n",
      "Epoch:    18 | Iteration:   400 | Loss: 1.44748\n",
      "Epoch:    18 | Iteration:   600 | Loss: 1.49369\n",
      "Epoch:    18 | Iteration:   800 | Loss: 1.61762\n",
      "Epoch:    18 | Iteration:  1000 | Loss: 1.41754\n",
      "Epoch:    18 | Iteration:  1200 | Loss: 1.56443\n",
      "Epoch:    18 | Iteration:  1400 | Loss: 1.59973\n",
      "Epoch:    18 | Iteration:  1600 | Loss: 1.61185\n",
      "\n",
      "Epoch:    18 | Loss: 1.46478\n",
      "\n",
      "model saved\n",
      "last model: epoch 18\n",
      "\n",
      "Validation Epoch:    18 | Loss: 1.38864\n",
      "\n",
      "Epoch:    19 | Iteration:     0 | Loss: 1.41549\n",
      "Epoch:    19 | Iteration:   200 | Loss: 1.39964\n",
      "Epoch:    19 | Iteration:   400 | Loss: 1.33924\n",
      "Epoch:    19 | Iteration:   600 | Loss: 1.51402\n",
      "Epoch:    19 | Iteration:   800 | Loss: 1.48148\n",
      "Epoch:    19 | Iteration:  1000 | Loss: 1.44978\n",
      "Epoch:    19 | Iteration:  1200 | Loss: 1.35111\n",
      "Epoch:    19 | Iteration:  1400 | Loss: 1.43654\n",
      "Epoch:    19 | Iteration:  1600 | Loss: 1.32771\n",
      "\n",
      "Epoch:    19 | Loss: 1.46475\n",
      "\n",
      "model saved\n",
      "last model: epoch 19\n",
      "\n",
      "Validation Epoch:    19 | Loss: 1.38769\n",
      "\n"
     ]
    }
   ],
   "source": [
    "running_loss = 0.0\n",
    "epoch_loss, min_epoch_loss = 0.0, 99.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    avg_loss = []\n",
    "    model.train()\n",
    "\n",
    "    for iter_, sample in enumerate(train_dataloader):\n",
    "\n",
    "        (h_in, c_in) = (torch.zeros(num_layers, batch_size, hidden_size, requires_grad=True).to(device),\n",
    "                        torch.zeros(num_layers, batch_size, hidden_size, requires_grad=True).to(device))   # 일단 제로로 텐서를 만들어 놓고 여기에 값을 model 결과값 튜플로 갱신할 것 \n",
    "\n",
    "        _, _, (input_data, output_data) = sample\n",
    "        \n",
    "        input_data = input_data.unsqueeze(0).to(device)\n",
    "        output_data = output_data.unsqueeze(0).to(device)\n",
    "\n",
    "        pred, (h_in, c_in) = model(input_data, h_in, c_in)\n",
    "        \n",
    "        loss = criterion(pred, output_data)\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_loss.append(loss.item())\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if iter_ % 200 == 199:\n",
    "            writer.add_scalar('training loss',\n",
    "                    running_loss / 200,\n",
    "                    epoch * len(train_dataloader) + iter_)\n",
    "            running_loss = 0\n",
    "\n",
    "        if iter_ % print_iter == 0:\n",
    "            print('Epoch: {:5} | Iteration: {:5} | Loss: {:1.5f}'.format(epoch, iter_, loss))\n",
    "\n",
    "    scheduler.step()\n",
    "    print('\\nEpoch: {:5} | Loss: {:1.5f}\\n'.format(epoch, sum(avg_loss) / len(avg_loss)))\n",
    "    avg_loss = []\n",
    "    \n",
    "    if epoch_loss < min_epoch_loss:\n",
    "        save_model('best', model, optimizer, scheduler)\n",
    "        print(f'best model so far: epoch {epoch}')\n",
    "        min_epoch_loss = epoch_loss\n",
    "\n",
    "    if epoch % save_epoch == 0:\n",
    "        save_model('last', model, optimizer, scheduler)\n",
    "        print(f'last model: epoch {epoch}')\n",
    "\n",
    "    if epoch % val_epoch == 0:\n",
    "        \n",
    "        val_score = 0.0\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            (h_in, c_in) = (torch.zeros(num_layers, 1, hidden_size, requires_grad=False).to(device),\n",
    "                    torch.zeros(num_layers, 1, hidden_size, requires_grad=False).to(device))\n",
    "\n",
    "            for iter_, sample in enumerate(validate_dataloader):\n",
    "\n",
    "                _, _, (input_data, output_data) = sample\n",
    "\n",
    "                input_data = input_data.unsqueeze(0).to(device)\n",
    "                output_data = output_data.unsqueeze(0).to(device)\n",
    "\n",
    "                pred, (h_in, c_in) = model(input_data, h_in, c_in)\n",
    "                score = criterion(pred, output_data)\n",
    "                val_score += score.item()\n",
    "        \n",
    "        print('\\nValidation Epoch: {:5} | Loss: {:1.5f}\\n'.format(epoch, val_score/len(validate_dataloader)))\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "seq_len = 7\n",
    "\n",
    "input_size = seq_len * 24\n",
    "hidden_size = 1024\n",
    "output_size = input_size\n",
    "batch_size = 1\n",
    "num_layers = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader, _ = data_loader(root=DATASET_PATH,\n",
    "                                  phase='test',\n",
    "                                  batch_size=batch_size,\n",
    "                                  seq_len=seq_len,\n",
    "                                  drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n"
     ]
    }
   ],
   "source": [
    "model = LSTMNet(input_size=input_size,\n",
    "                hidden_size=hidden_size,\n",
    "                output_size=output_size,\n",
    "                batch_size=batch_size,\n",
    "                num_layers=num_layers)\n",
    "\n",
    "# model\n",
    "model_name = 'log/best.pth'\n",
    "\n",
    "load_model(model_name, model)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_file_path = os.path.join(DATASET_PATH, 'sample_submission.csv')\n",
    "submission_table = pd.read_csv(submission_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "(h_in, c_in) = (torch.zeros(num_layers, 1, hidden_size, requires_grad=False).to(device),\n",
    "                torch.zeros(num_layers, 1, hidden_size, requires_grad=False).to(device))\n",
    "\n",
    "for iter_, sample in enumerate(test_dataloader):\n",
    "\n",
    "    timestamp, category, (input_data, output_data) = sample\n",
    "    input_data = input_data.unsqueeze(0).to(device)\n",
    "\n",
    "    pred, (h_in, c_in) = model(input_data, h_in, c_in)\n",
    "\n",
    "    for i, (t, h) in enumerate(zip(timestamp[0], timestamp[1])):\n",
    "        for cat, row in zip(category, pred[0]):\n",
    "            cat = f'road_{cat}'\n",
    "            submission_table[cat] = row.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 셀을 실행하면 기존에 있던 prediction.csv가 덮어씌워집니다. 필요하시면 코드를 수정하거나 혹은 prediction.csv를 백업하시기 바랍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_table.to_csv('prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
