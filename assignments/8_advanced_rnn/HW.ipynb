{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"HW.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM6MIZwy2jz+nSuKzz9EiPZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"LKQbNYOP2HKg"},"source":["# HW 간단한 Text 예측 GRU 모델 코드 완성하기"]},{"cell_type":"code","metadata":{"id":"p1U7voaP2LQN"},"source":["\"\"\"\n","아래 ??? 로 표시된 부분의 코드를 작성해서 아래 코드가 작동하게 하시면 됩니다. \n","\"\"\"\n","\n","sentences = [\"i like dog\", \"i love coffee\", \"i hate milk\", \"you like cat\", \"you love milk\", \"you hate coffee\"]\n","dtype = torch.float\n","\n","\"\"\"\n","Word Processing\n","\"\"\"\n","word_list = list(set(\" \".join(sentences).split()))\n","word_dict = {w: i for i, w in enumerate(word_list)}\n","number_dict = {i: w for i, w in enumerate(word_list)}\n","n_class = len(word_dict)\n","# \"\"\"\n","# TextRNN Parameter\n","# \"\"\"\n","batch_size = len(sentences)\n","n_step = 2  # 학습 하려고 하는 문장의 길이 - 1\n","n_hidden = 5  # 은닉층 사이즈\n","\n","def make_batch(sentences):\n","  input_batch = []\n","  target_batch = []\n","\n","  for sen in sentences:\n","    word = sen.split()\n","    input = [word_dict[n] for n in word[:-1]]\n","    target = word_dict[word[-1]]\n","\n","    input_batch.append(np.eye(n_class)[input])  # One-Hot Encoding\n","    target_batch.append(target)\n","  \n","  return input_batch, target_batch\n","\n","input_batch, target_batch = make_batch(sentences)\n","input_batch = torch.tensor(input_batch, dtype=torch.float32, requires_grad=True)\n","target_batch = torch.tensor(target_batch, dtype=torch.int64)\n","# \"\"\"\n","# TextRNN\n","# \"\"\"\n","class TextRNN(nn.Module):\n","  def __init__(self):\n","    super(TextRNN, self).__init__()\n","    \"\"\"\n","    TODO LSTM 모델 구축 \n","    \"\"\"\n","    self.gru = ???\n","    self.W = nn.Parameter(torch.randn([ n_hidden, n_class]).type(dtype))\n","    self.b = nn.Parameter(torch.randn([n_class]).type(dtype))\n","    self.Softmax = nn.Softmax(dim=1)\n","\n","  def forward(self, X):\n","    # X = X.transpose(0, 1)\n","    \"\"\"\n","    TODO GRU Hidden state 구축 및 최종 예측 hiddden state 를 뽑기 \n","    \"\"\"\n","    ???\n","    ???\n","    model = torch.mm(outputs, self.W) + self.b  # 최종 예측 최종 출력 층\n","    return model\n","\t\n","\n","# \"\"\"\n","# Training\n","# \"\"\"\n","model = TextRNN()\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n","\n","for epoch in range(100):\n","  output = model(input_batch)\n","  loss = criterion(output, target_batch)\n","\n","  if (epoch + 1) % 100 == 0:\n","    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n","  \n","  optimizer.zero_grad()\n","  loss.backward()\n","  optimizer.step()\n","\n","input = [sen.split()[:2] for sen in sentences]\n","\n","hidden = torch.zeros(1, batch_size, n_hidden, requires_grad=True)\n","predict = model(input_batch).data.max(1, keepdim=True)[1]\n","print([sen.split()[:2] for sen in sentences], '->', [number_dict[n.item()] for n in predict.squeeze()])\n"],"execution_count":null,"outputs":[]}]}